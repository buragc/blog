+++
title = "Dynamic resource allocation in your data center"
date = "2014-01-08"
description = "Shifting resources to functions that need it"
showcomments = true
tags = ["enterprise", "engineering", "services"]
+++

By now all of us architects are very much used to the idea of spinning a new server in the cloud and scaling our solution horizontally as needed. Virtually nonexistent setup times and a good API provided by cloud vendors makes this a possibility. Infrastructure has come a long way in the form of IAAS and PAAS. Also, open source software has been a great enabler of this movement; namely deployment automation technologies and linux distributions that are customized per use case. The situation in most enterprise systems however, is not that great. It is not rare for me to run into a horizontally scaled solution that is not efficiently utilizing the computing resources. Take the following example; a software solution that serves an end to end business process is created as a single service. When the volume increases on the client, the service is scaled by creating an exact replica and using a load balancer to reduce the load. To elaborate on the issue at hand, I will use a hypothetical car insurance company. As you may already know, there are some basic concepts in car insurance business such as finding a quote that fits your needs, comparing them with competitors, signing up for a policy and finally paying for it. If everything goes well and you don't get into an accident, your interaction with the insurance company may just end there. Their "service" oriented software may be running on a server that looks like the following: [![Image](http://buragc.files.wordpress.com/2014/01/drawing0.png?w=482) ](http://buragc.files.wordpress.com/2014/01/drawing0.png) When this company starts becoming successful and gets **permanent growth, **the current computing capacity may no longer serve its needs. What I mean by permanent growth is a net gain on the  number of users. Say, from X to 2X. This growth is a pretty happy and desirable scenario which can be the result of geographic expansion, an acquisition or a marketing campaign. Any sane architect would just double the capacity, distribute the traffic with a load balancer go on with life. The server stacks may look something like this after the capacity upgrade: [![Image](http://buragc.files.wordpress.com/2014/01/drawing1.png?w=650)](http://buragc.files.wordpress.com/2014/01/drawing1.png) While this is going on, an X-Ray of the business processes may show us some service resource usage distribution that looks like this. [![Image](http://buragc.files.wordpress.com/2014/01/drawing2.png?w=650)](http://buragc.files.wordpress.com/2014/01/drawing2.png) Now lets think about a slightly more interesting scenario. Hurricane Sandy happens. A lot of cars are damaged and as a result the organization is experiencing a spike in the number of claim requests it gets. Since the servers are tuned to handle the current capacity (with a foreseen +/- 5% elasticity) things start slowing down. Customers who are on the phone with the customer service reps start experiencing a longer wait time. Mobile phone based claim submission requests start timing out. Overall, customer satisfaction goes down and there is very little this organization can do about it because they cannot procure and deploy new services overnight and configure their client software to handle the spike. If they had access to cloud resources, they could certainly grow the number of servers temporarily into the cloud and turn them down later. However, in the absence of that there is not much this organization can do. This is exactly why a DRA like framework is needed in the enterprise. Imagine, if this organization could temporarily limit (or even turn off) its capacity to sell more policies and re-allocate those computing resources to the business function that it needs at the moment. The service allocation could look something like this: [![Image](http://buragc.files.wordpress.com/2014/01/drawing4.png?w=650)](http://buragc.files.wordpress.com/2014/01/drawing4.png) This re-configuration could save the company thousands of unhappy customers and more importantly ensure that the business resources were utilized to the maximum when they were needed. This concept is not entirely new. There are similarities to Cory Isacson's [Software Pipelines and SOA: Releasing the Power of Multi-Core Processing](http://www.amazon.com/gp/product/0137137974/ref=as_li_ss_tl?ie=UTF8&camp=1789&creative=390957&creativeASIN=0137137974&linkCode=as2&tag=watchesunder200-20)![](http://ir-na.amazon-adsystem.com/e/ir?t=watchesunder200-20&l=as2&o=1&a=0137137974)Software Pipelines and SOA and cloud computing in general. Some advanced organization with very good engineering teams are able to achieve this scenario by maximizing technologies like Chef/Puppet in the cloud. However, it is not an easily accessible to the common enterprise. What if there was an application/services framework that facilitated this? I believe that a solution like this would truly align the business' needs with IT capabilities of the enterprise.
